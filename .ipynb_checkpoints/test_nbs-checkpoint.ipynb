{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-firewall",
   "metadata": {
    "id": "VN7e23DXUu8y"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ShivamJoshi64/tototodo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-hotel",
   "metadata": {
    "id": "_GnLDkHSVUO5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./tototodo/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confused-brook",
   "metadata": {
    "id": "aF_iNqFWUqdW"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-69d1136b0060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from resnet_upsample import ResNet, ResidualBlock, Bottleneck\n",
    "# from resnet_upsample3x3 import ResNet, ResidualBlock, Bottleneck\n",
    "# from inception3 import *\n",
    "# from inceptionv4 import *\n",
    "#from model.vgg_net import *\n",
    "# from alex_net import AlexNet\n",
    "# \n",
    "# res_net_use_this, csinet1.0\n",
    "from model.res_net_use_this import ResNet, ResidualBlock, Bottleneck\n",
    "# resnet_generation_upsample, csinet1.5\n",
    "# from resnet_generation_upsample import ResNet, ResidualBlock, Bottleneck\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-heading",
   "metadata": {
    "id": "U0dkG1_mUqdb"
   },
   "outputs": [],
   "source": [
    "# load train data\n",
    "data = sio.loadmat('dataset1.mat')\n",
    "train_data = data['train']\n",
    "train_label = data['train_labels']\n",
    "\n",
    "# label matrix organized as nSamplex5, where the 1st coloum is the index of personID, the latter 4 are 4 biometrcs\n",
    "## train_label[:, 0] = train_label[:, 0] - 1 # 1--30 -> 0--29\n",
    "\n",
    "num_train_instances = len(train_data)\n",
    "\n",
    "# prepare data, nSample x nChannel x width x height\n",
    "# reshape train data size to nSample x nSubcarrier x 1 x 1\n",
    "train_data = torch.from_numpy(train_data).type(torch.FloatTensor).view(num_train_instances, 64, 1, 1)\n",
    "train_label = torch.from_numpy(train_label).type(torch.FloatTensor)\n",
    "train_dataset = TensorDataset(train_data, train_label)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-halifax",
   "metadata": {
    "id": "QBj2iPD4Uqdc"
   },
   "outputs": [],
   "source": [
    "# load test data\n",
    "data = sio.loadmat('test_dataset1.mat')\n",
    "test_data = data['test_data']\n",
    "test_label = data['test_label']\n",
    "#test_label[:, 0] = test_label[:, 0] - 1\n",
    "\n",
    "num_test_instances = len(test_data)\n",
    "# prepare data, nSample x nChannel x width x height\n",
    "# reshape test data size to nSample x nSubcarrier x 1 x 1\n",
    "test_data = torch.from_numpy(test_data).type(torch.FloatTensor).view(num_test_instances, 64, 1, 1)\n",
    "test_label = torch.from_numpy(test_label).type(torch.FloatTensor)\n",
    "test_dataset = TensorDataset(test_data, test_label)\n",
    "test_data_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#print(test_data_loader.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-warner",
   "metadata": {
    "id": "m1Ku0Su8Uqdd"
   },
   "outputs": [],
   "source": [
    "resnet = ResNet(ResidualBlock, [2, 2, 2, 2], 2) # last param = no. of classes\n",
    "# resnet = ResNet(ResidualBlock, [3, 4, 6, 3], 30)\n",
    "# resnet = ResNet(Bottleneck, [3, 4, 6, 3], 10)\n",
    "# resnet = ResNet(Bottleneck, [3, 4, 23, 3], 30)\n",
    "# inception = InceptionV4(30)\n",
    "#vgg = VGG(make_layers(cfg['E'], batch_norm=True))\n",
    "# alexnet = AlexNet().cuda()\n",
    "#resnet = vgg.cuda() # vgg = vgg.cuda() originally\n",
    "# alexnet = alexnet.cuda()\n",
    "# alexnet.eval()\n",
    "\n",
    "resnet = resnet.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-strip",
   "metadata": {
    "id": "VW_5g4QMUqdd"
   },
   "outputs": [],
   "source": [
    "criterion1 = nn.CrossEntropyLoss().cuda()\n",
    "#criterion2 = nn.L1Loss().cuda()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6, 9, 12, 15, 18], gamma=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-honey",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWEJ8oGuUqdd",
    "outputId": "6fb0da56-2b0a-43c1-c5cd-cf41a805e55b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "  0%|          | 2/400 [00:00<00:22, 17.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:27<00:00, 14.61it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 47.73it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:01, 54.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 75.4125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.10it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 70.7\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:27<00:00, 14.62it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 47.79it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:01, 54.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 68.6125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.10it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 75.85\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:27<00:00, 14.61it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 47.77it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:01, 54.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 90.4875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.17it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 92.15\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:27<00:00, 14.61it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 47.77it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:01, 54.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 96.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.13it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.4\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:27<00:00, 14.61it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 47.76it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:01, 54.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 96.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.11it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.45\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:27<00:00, 14.61it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 47.77it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:01, 54.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.2625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.13it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.15\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:27<00:00, 14.61it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 47.76it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:01, 54.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.7375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.14it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.4\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:27<00:00, 14.61it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 47.72it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:01, 54.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.10it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.7\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:27<00:00, 14.59it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 47.76it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:01, 54.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.14it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.2\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:27<00:00, 14.62it/s]\n",
      "100%|██████████| 400/400 [00:08<00:00, 47.77it/s]\n",
      "  6%|▌         | 6/100 [00:00<00:01, 54.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98.45\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch:', epoch)\n",
    "    resnet.train()\n",
    "\n",
    "    scheduler.step()\n",
    "    # trained_num = 0\n",
    "    for (samples, labels) in tqdm(train_data_loader):\n",
    "\n",
    "        # sample_len = len(samples)\n",
    "        # trained_num += sample_len\n",
    "        # print('Process', 100*trained_num/num_train_instances)\n",
    "\n",
    "        samplesV = Variable(samples.cuda())\n",
    "        labels = labels.squeeze()\n",
    "        labelsV = Variable(labels.cuda())\n",
    "        \n",
    "        #print(len(samplesV)) # 20 batch size\n",
    "        #print(labelsV) # size = 20 = batch size\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        predict_label = resnet(samplesV)     \n",
    "        #print(len(predict_label),len(predict_label[0])) # 2,20\n",
    "        \n",
    "        #print(predict_label[0])\n",
    "        #print(labelsV) # ERROR ZONE\n",
    "        \n",
    "        #lossC = criterion1(predict_label[0], labelsV[:, 0].type(torch.LongTensor).cuda())\n",
    "        lossC = criterion1(predict_label[0], labelsV.type(torch.LongTensor).cuda())\n",
    "\n",
    "        '''\n",
    "        lossR1 = criterion2(predict_label[1][:, 0], labelsV)\n",
    "        lossR2 = criterion2(predict_label[1][:, 1], labelsV)\n",
    "        lossR3 = criterion2(predict_label[1][:, 2], labelsV)\n",
    "        lossR4 = criterion2(predict_label[1][:, 3], labelsV)\n",
    "        '''\n",
    "        \n",
    "        #loss = lossC + (0.0386*lossR1 + 0.0405*lossR2 + 0.0629*lossR3 + 0.0877*lossR4)/4\n",
    "        loss = lossC\n",
    "        # Why 0.0386, 0.0405, 0.06029 and 0.0877: these fours are used to normalize four body biometrics\n",
    "        # fat/muscle/water/bone rates, for example, if looking paper Table 6, where we showed the information of \n",
    "        # 30 recruited subjects. The minimal fat rate is 5, the maximum is 30.9, we decided to normarlize the fat rate\n",
    "        # by dividing (31-5), resulting in 0.0386.  0.0405->[65,90]muscle rate, 0.0629->[49,65]water rate, 0.0877->[1.5 13.0]  \n",
    "        # We doing this was spired by Faster RCNN loss, which has a object classfication and a bounding box regression. As paper\n",
    "        # said, they normalized the regression loss.\n",
    "        #\n",
    "        # print(loss.item())\n",
    "        # loss_every += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "# #\n",
    "    resnet.eval()\n",
    "    correct_t = 0\n",
    "    for (samples, labels) in tqdm(train_data_loader):\n",
    "        with torch.no_grad():\n",
    "            samplesV = Variable(samples.cuda())\n",
    "            labelsV = Variable(labels.cuda())\n",
    "            # labelsV = labelsV.view(-1)\n",
    "\n",
    "            predict_label = resnet(samplesV)\n",
    "            prediction = predict_label[0].data.max(1)[1]\n",
    "            correct_t += prediction.eq(labelsV[:, 0].data.long()).sum()\n",
    "\n",
    "    print(\"Training accuracy:\", (100*float(correct_t)/num_train_instances))\n",
    "\n",
    "    trainacc = str(100*float(correct_t)/num_train_instances)[0:6]\n",
    "\n",
    "    correct_t = 0\n",
    "    for (samples, labels) in tqdm(test_data_loader):\n",
    "        with torch.no_grad():\n",
    "            samplesV = Variable(samples.cuda())\n",
    "            labelsV = Variable(labels.cuda())\n",
    "            # labelsV = labelsV.view(-1)\n",
    "\n",
    "        predict_label = resnet(samplesV)\n",
    "        prediction = predict_label[0].data.max(1)[1]\n",
    "        correct_t += prediction.eq(labelsV[:, 0].data.long()).sum()\n",
    "\n",
    "    print(\"Test accuracy:\", (100 * float(correct_t) / num_test_instances))\n",
    "\n",
    "    testacc = str(100 * float(correct_t) / num_test_instances)[0:6]\n",
    "\n",
    "    torch.save(resnet, './weights/resnet18_Train' + trainacc + 'Test' + testacc + '.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "test-nbs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
