{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VN7e23DXUu8y"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ShivamJoshi64/tototodo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GnLDkHSVUO5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./tototodo/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aF_iNqFWUqdW"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from resnet_upsample import ResNet, ResidualBlock, Bottleneck\n",
    "# from resnet_upsample3x3 import ResNet, ResidualBlock, Bottleneck\n",
    "# from inception3 import *\n",
    "# from inceptionv4 import *\n",
    "#from model.vgg_net import *\n",
    "# from alex_net import AlexNet\n",
    "# \n",
    "# res_net_use_this, csinet1.0\n",
    "from model.res_net_use_this import ResNet, ResidualBlock, Bottleneck\n",
    "# resnet_generation_upsample, csinet1.5\n",
    "# from resnet_generation_upsample import ResNet, ResidualBlock, Bottleneck\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "U0dkG1_mUqdb"
   },
   "outputs": [],
   "source": [
    "# load train data\n",
    "data = sio.loadmat('datasets/dataset1.mat')\n",
    "train_data = data['train_data']\n",
    "train_label = data['train_labels']\n",
    "\n",
    "# label matrix organized as nSamplex5, where the 1st coloum is the index of personID, the latter 4 are 4 biometrcs\n",
    "## train_label[:, 0] = train_label[:, 0] - 1 # 1--30 -> 0--29\n",
    "\n",
    "num_train_instances = len(train_data)\n",
    "\n",
    "# prepare data, nSample x nChannel x width x height\n",
    "# reshape train data size to nSample x nSubcarrier x 1 x 1\n",
    "train_data = torch.from_numpy(train_data).type(torch.FloatTensor).view(num_train_instances, 64, 1, 1)\n",
    "train_label = torch.from_numpy(train_label).type(torch.FloatTensor)\n",
    "train_dataset = TensorDataset(train_data, train_label)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QBj2iPD4Uqdc"
   },
   "outputs": [],
   "source": [
    "# load test data\n",
    "data = sio.loadmat('datasets/test_dataset1.mat')\n",
    "test_data = data['test_data']\n",
    "test_label = data['test_labels']\n",
    "#test_label[:, 0] = test_label[:, 0] - 1\n",
    "\n",
    "num_test_instances = len(test_data)\n",
    "# prepare data, nSample x nChannel x width x height\n",
    "# reshape test data size to nSample x nSubcarrier x 1 x 1\n",
    "test_data = torch.from_numpy(test_data).type(torch.FloatTensor).view(num_test_instances, 64, 1, 1)\n",
    "test_label = torch.from_numpy(test_label).type(torch.FloatTensor)\n",
    "test_dataset = TensorDataset(test_data, test_label)\n",
    "test_data_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#print(test_data_loader.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1Ku0Su8Uqdd"
   },
   "outputs": [],
   "source": [
    "resnet = ResNet(ResidualBlock, [2, 2, 2, 2], 2) # last param = no. of classes\n",
    "# resnet = ResNet(ResidualBlock, [3, 4, 6, 3], 30)\n",
    "# resnet = ResNet(Bottleneck, [3, 4, 6, 3], 10)\n",
    "# resnet = ResNet(Bottleneck, [3, 4, 23, 3], 30)\n",
    "# inception = InceptionV4(30)\n",
    "#vgg = VGG(make_layers(cfg['E'], batch_norm=True))\n",
    "# alexnet = AlexNet().cuda()\n",
    "#resnet = vgg.cuda() # vgg = vgg.cuda() originally\n",
    "# alexnet = alexnet.cuda()\n",
    "# alexnet.eval()\n",
    "\n",
    "resnet = resnet.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VW_5g4QMUqdd"
   },
   "outputs": [],
   "source": [
    "criterion1 = nn.CrossEntropyLoss().cuda()\n",
    "#criterion2 = nn.L1Loss().cuda()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 6, 9, 12, 15, 18], gamma=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWEJ8oGuUqdd",
    "outputId": "6fb0da56-2b0a-43c1-c5cd-cf41a805e55b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/anaconda3/envs/dl/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "\r",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:42<00:00,  9.51it/s]\n",
      "100%|██████████| 400/400 [00:11<00:00, 34.36it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 39.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 87.4125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 33.56it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 84.5\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:42<00:00,  9.47it/s]\n",
      "100%|██████████| 400/400 [00:11<00:00, 34.05it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 38.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 89.3375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 33.19it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 88.75\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:42<00:00,  9.49it/s]\n",
      "100%|██████████| 400/400 [00:11<00:00, 33.61it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 39.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 75.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 33.11it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 78.6\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:42<00:00,  9.45it/s]\n",
      "100%|██████████| 400/400 [00:11<00:00, 33.50it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 38.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 93.425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 33.12it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 93.95\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:42<00:00,  9.37it/s]\n",
      "100%|██████████| 400/400 [00:11<00:00, 33.87it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 39.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 94.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 33.10it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 94.3\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:42<00:00,  9.39it/s]\n",
      "100%|██████████| 400/400 [00:11<00:00, 33.85it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 38.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 33.06it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.8\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:42<00:00,  9.36it/s]\n",
      "100%|██████████| 400/400 [00:11<00:00, 33.84it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 39.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 95.2875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 33.08it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 95.8\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:42<00:00,  9.37it/s]\n",
      "100%|██████████| 400/400 [00:11<00:00, 33.84it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 39.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.0375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 33.00it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.4\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:42<00:00,  9.36it/s]\n",
      "100%|██████████| 400/400 [00:11<00:00, 33.79it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 38.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.6375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 33.03it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.6\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:42<00:00,  9.37it/s]\n",
      "100%|██████████| 400/400 [00:11<00:00, 33.77it/s]\n",
      "  5%|▌         | 5/100 [00:00<00:02, 40.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 34.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.85\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch:', epoch)\n",
    "    resnet.train()\n",
    "\n",
    "    scheduler.step()\n",
    "    # trained_num = 0\n",
    "    for (samples, labels) in tqdm(train_data_loader):\n",
    "\n",
    "        # sample_len = len(samples)\n",
    "        # trained_num += sample_len\n",
    "        # print('Process', 100*trained_num/num_train_instances)\n",
    "\n",
    "        samplesV = Variable(samples.cuda())\n",
    "        labels = labels.squeeze()\n",
    "        labelsV = Variable(labels.cuda())\n",
    "        \n",
    "        #print(len(samplesV)) # 20 batch size\n",
    "        #print(labelsV) # size = 20 = batch size\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        predict_label = resnet(samplesV)     \n",
    "        #print(len(predict_label),len(predict_label[0])) # 2,20\n",
    "        \n",
    "        #print(predict_label[0])\n",
    "        #print(labelsV) # ERROR ZONE\n",
    "        \n",
    "        #lossC = criterion1(predict_label[0], labelsV[:, 0].type(torch.LongTensor).cuda())\n",
    "        lossC = criterion1(predict_label[0], labelsV.type(torch.LongTensor).cuda())\n",
    "\n",
    "        '''\n",
    "        lossR1 = criterion2(predict_label[1][:, 0], labelsV)\n",
    "        lossR2 = criterion2(predict_label[1][:, 1], labelsV)\n",
    "        lossR3 = criterion2(predict_label[1][:, 2], labelsV)\n",
    "        lossR4 = criterion2(predict_label[1][:, 3], labelsV)\n",
    "        '''\n",
    "        \n",
    "        #loss = lossC + (0.0386*lossR1 + 0.0405*lossR2 + 0.0629*lossR3 + 0.0877*lossR4)/4\n",
    "        loss = lossC\n",
    "        # Why 0.0386, 0.0405, 0.06029 and 0.0877: these fours are used to normalize four body biometrics\n",
    "        # fat/muscle/water/bone rates, for example, if looking paper Table 6, where we showed the information of \n",
    "        # 30 recruited subjects. The minimal fat rate is 5, the maximum is 30.9, we decided to normarlize the fat rate\n",
    "        # by dividing (31-5), resulting in 0.0386.  0.0405->[65,90]muscle rate, 0.0629->[49,65]water rate, 0.0877->[1.5 13.0]  \n",
    "        # We doing this was spired by Faster RCNN loss, which has a object classfication and a bounding box regression. As paper\n",
    "        # said, they normalized the regression loss.\n",
    "        #\n",
    "        # print(loss.item())\n",
    "        # loss_every += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "# #\n",
    "    resnet.eval()\n",
    "    correct_t = 0\n",
    "    for (samples, labels) in tqdm(train_data_loader):\n",
    "        with torch.no_grad():\n",
    "            samplesV = Variable(samples.cuda())\n",
    "            labelsV = Variable(labels.cuda())\n",
    "            # labelsV = labelsV.view(-1)\n",
    "\n",
    "            predict_label = resnet(samplesV)\n",
    "            prediction = predict_label[0].data.max(1)[1]\n",
    "            correct_t += prediction.eq(labelsV[:, 0].data.long()).sum()\n",
    "\n",
    "    print(\"Training accuracy:\", (100*float(correct_t)/num_train_instances))\n",
    "\n",
    "    trainacc = str(100*float(correct_t)/num_train_instances)[0:6]\n",
    "\n",
    "    correct_t = 0\n",
    "    for (samples, labels) in tqdm(test_data_loader):\n",
    "        with torch.no_grad():\n",
    "            samplesV = Variable(samples.cuda())\n",
    "            labelsV = Variable(labels.cuda())\n",
    "            # labelsV = labelsV.view(-1)\n",
    "\n",
    "        predict_label = resnet(samplesV)\n",
    "        prediction = predict_label[0].data.max(1)[1]\n",
    "        correct_t += prediction.eq(labelsV[:, 0].data.long()).sum()\n",
    "\n",
    "    print(\"Test accuracy:\", (100 * float(correct_t) / num_test_instances))\n",
    "\n",
    "    testacc = str(100 * float(correct_t) / num_test_instances)[0:6]\n",
    "\n",
    "torch.save(resnet, './weights/resnet18_Train' + trainacc + 'Test' + testacc + 'dataset1.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "test-nbs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
